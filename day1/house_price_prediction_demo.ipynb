{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Demo D·ª± ƒêo√°n Gi√° Nh√† - Machine Learning vs Deep Learning\n",
        "\n",
        "## M·ª•c ti√™u:\n",
        "- So s√°nh hi·ªáu qu·∫£ gi·ªØa Machine Learning truy·ªÅn th·ªëng v√† Deep Learning\n",
        "- Th·ª±c h√†nh t·ª´ thu th·∫≠p d·ªØ li·ªáu ‚Üí hu·∫•n luy·ªán ‚Üí test ‚Üí ƒë√°nh gi√°\n",
        "- Hi·ªÉu c√°ch c√°c model ho·∫°t ƒë·ªông trong th·ª±c t·∫ø\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. T·∫°o D·ªØ Li·ªáu Gi·∫£ L·∫≠p (Realistic Data)\n",
        "Trong th·ª±c t·∫ø, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c trang web b·∫•t ƒë·ªông s·∫£n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p d·ª±a tr√™n th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n Vi·ªát Nam\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# C√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn gi√° nh√†\n",
        "data = {\n",
        "    'dien_tich': np.clip(np.random.normal(80, 25, n_samples), 30, 200),  # 30-200 m2\n",
        "    'so_phong_ngu': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.1, 0.3, 0.4, 0.15, 0.05]),\n",
        "    'so_phong_tam': np.random.choice([1, 2, 3], n_samples, p=[0.4, 0.5, 0.1]),\n",
        "    'khoang_cach_trung_tam': np.clip(np.random.exponential(8, n_samples), 1, 30),  # 1-30 km\n",
        "    'tuoi_nha': np.random.randint(0, 25, n_samples),  # 0-25 nƒÉm\n",
        "    'gan_truong_hoc': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
        "    'gan_benh_vien': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "    'co_ho_boi': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
        "    'co_thang_may': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# T·∫°o c√¥ng th·ª©c t√≠nh gi√° nh√† d·ª±a tr√™n logic th·ª±c t·∫ø\n",
        "def tinh_gia_nha(row):\n",
        "    gia_co_so = 50_000_000  # 50 tri·ªáu VND/m2\n",
        "    \n",
        "    # Y·∫øu t·ªë ch√≠nh: di·ªán t√≠ch\n",
        "    gia = gia_co_so * row['dien_tich']\n",
        "    \n",
        "    # Ph√≤ng ng·ªß v√† ph√≤ng t·∫Øm\n",
        "    gia += row['so_phong_ngu'] * 150_000_000  # Gi·∫£m xu·ªëng 150tr/ph√≤ng\n",
        "    gia += row['so_phong_tam'] * 80_000_000   # Gi·∫£m xu·ªëng 80tr/ph√≤ng t·∫Øm\n",
        "    \n",
        "    # V·ªã tr√≠ (c√†ng xa trung t√¢m c√†ng r·∫ª) - gi·∫£m impact ƒë·ªÉ tr√°nh √¢m\n",
        "    distance_factor = max(0.3, 1 - row['khoang_cach_trung_tam'] * 0.015)  # T·ªëi thi·ªÉu 30%\n",
        "    gia *= distance_factor\n",
        "    \n",
        "    # Tu·ªïi nh√† (c√†ng c≈© c√†ng r·∫ª) - gi·∫£m impact\n",
        "    age_factor = max(0.5, 1 - row['tuoi_nha'] * 0.008)  # T·ªëi thi·ªÉu 50%\n",
        "    gia *= age_factor\n",
        "    \n",
        "    # Ti·ªán √≠ch xung quanh\n",
        "    if row['gan_truong_hoc']: gia *= 1.08\n",
        "    if row['gan_benh_vien']: gia *= 1.05\n",
        "    if row['co_ho_boi']: gia *= 1.15\n",
        "    if row['co_thang_may']: gia *= 1.1\n",
        "    \n",
        "    # ƒê·∫£m b·∫£o gi√° kh√¥ng √¢m tr∆∞·ªõc khi th√™m noise\n",
        "    gia = max(gia, 500_000_000)  # T·ªëi thi·ªÉu 500 tri·ªáu\n",
        "    \n",
        "    # Th√™m noise nh·ªè h∆°n ƒë·ªÉ tr√°nh gi√° tr·ªã √¢m\n",
        "    noise = np.random.normal(0, gia * 0.05)  # Gi·∫£m t·ª´ 0.1 xu·ªëng 0.05\n",
        "    gia += noise\n",
        "    \n",
        "    return max(gia, 500_000_000)  # ƒê·∫£m b·∫£o gi√° t·ªëi thi·ªÉu\n",
        "\n",
        "df['gia_nha'] = df.apply(tinh_gia_nha, axis=1)\n",
        "\n",
        "print(\"üìä D·ªØ li·ªáu m·∫´u:\")\n",
        "print(df.head())\n",
        "print(f\"\\nüìà T·ªïng s·ªë m·∫´u: {len(df)}\")\n",
        "print(f\"üí∞ Gi√° nh√† trung b√¨nh: {df['gia_nha'].mean():,.0f} VND\")\n",
        "print(f\"üí∞ Gi√° nh√† cao nh·∫•t: {df['gia_nha'].max():,.0f} VND\")\n",
        "print(f\"üí∞ Gi√° nh√† th·∫•p nh·∫•t: {df['gia_nha'].min():,.0f} VND\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Kh√°m Ph√° D·ªØ Li·ªáu (Data Exploration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì kh√°m ph√° d·ªØ li·ªáu\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df['gia_nha']/1_000_000_000, bins=30, alpha=0.7, color='skyblue')\n",
        "plt.title('Ph√¢n B·ªë Gi√° Nh√†')\n",
        "plt.xlabel('Gi√° (T·ª∑ VND)')\n",
        "plt.ylabel('S·ªë l∆∞·ª£ng')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df['dien_tich'], df['gia_nha']/1_000_000_000, alpha=0.5)\n",
        "plt.title('Di·ªán T√≠ch vs Gi√° Nh√†')\n",
        "plt.xlabel('Di·ªán t√≠ch (m¬≤)')\n",
        "plt.ylabel('Gi√° (T·ª∑ VND)')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df['khoang_cach_trung_tam'], df['gia_nha']/1_000_000_000, alpha=0.5, color='orange')\n",
        "plt.title('Kho·∫£ng C√°ch Trung T√¢m vs Gi√°')\n",
        "plt.xlabel('Kho·∫£ng c√°ch (km)')\n",
        "plt.ylabel('Gi√° (T·ª∑ VND)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chu·∫©n B·ªã D·ªØ Li·ªáu + Training Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "X = df.drop('gia_nha', axis=1)\n",
        "y = df['gia_nha']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Chu·∫©n h√≥a d·ªØ li·ªáu\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"üìä D·ªØ li·ªáu training: {X_train.shape}\")\n",
        "print(f\"üìä D·ªØ li·ªáu testing: {X_test.shape}\")\n",
        "\n",
        "# Training Machine Learning Models\n",
        "print(\"\\nüî® Training Machine Learning models...\")\n",
        "\n",
        "# 1. Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Deep Learning Model\n",
        "print(\"üß† Training Deep Learning model...\")\n",
        "dl_model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "dl_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = dl_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, \n",
        "                      validation_split=0.2, verbose=0)\n",
        "\n",
        "print(\"‚úÖ T·∫•t c·∫£ models ƒë√£ ƒë∆∞·ª£c train xong!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. TEST V√Ä ƒê√ÅNH GI√Å MODELS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# D·ª± ƒëo√°n tr√™n test set\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "dl_pred = dl_model.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "# H√†m ƒë√°nh gi√° model\n",
        "def danh_gia_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"üìä {model_name}:\")\n",
        "    print(f\"   MAE: {mae:,.0f} VND ({(mae/y_true.mean())*100:.1f}%)\")\n",
        "    print(f\"   RMSE: {rmse:,.0f} VND\")\n",
        "    print(f\"   R¬≤ Score: {r2:.3f}\")\n",
        "    \n",
        "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Error_%': (mae/y_true.mean())*100}\n",
        "\n",
        "# ƒê√°nh gi√° t·ª´ng model\n",
        "results = {}\n",
        "results['Linear Regression'] = danh_gia_model(y_test, lr_pred, \"Linear Regression\")\n",
        "results['Random Forest'] = danh_gia_model(y_test, rf_pred, \"Random Forest\")\n",
        "results['Deep Learning'] = danh_gia_model(y_test, dl_pred, \"Deep Learning\")\n",
        "\n",
        "# B·∫£ng so s√°nh\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nüèÜ B·∫¢NG X·∫æP H·∫†NG:\")\n",
        "print(results_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. TEST TH·ª∞C T·∫æ - D·ª± ƒêo√°n Gi√° Nh√† M·ªõi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def du_doan_gia_nha(dien_tich, so_phong_ngu, so_phong_tam, khoang_cach_trung_tam, \n",
        "                   tuoi_nha, gan_truong_hoc, gan_benh_vien, co_ho_boi, co_thang_may):\n",
        "    \"\"\"H√†m d·ª± ƒëo√°n gi√° nh√† t·ª´ c√°c th√¥ng s·ªë\"\"\"\n",
        "    new_house = pd.DataFrame({\n",
        "        'dien_tich': [dien_tich], 'so_phong_ngu': [so_phong_ngu],\n",
        "        'so_phong_tam': [so_phong_tam], 'khoang_cach_trung_tam': [khoang_cach_trung_tam],\n",
        "        'tuoi_nha': [tuoi_nha], 'gan_truong_hoc': [gan_truong_hoc],\n",
        "        'gan_benh_vien': [gan_benh_vien], 'co_ho_boi': [co_ho_boi], 'co_thang_may': [co_thang_may]\n",
        "    })\n",
        "    \n",
        "    new_house_scaled = scaler.transform(new_house)\n",
        "    \n",
        "    lr_price = lr_model.predict(new_house)[0]\n",
        "    rf_price = rf_model.predict(new_house)[0]\n",
        "    dl_price = dl_model.predict(new_house_scaled, verbose=0)[0][0]\n",
        "    \n",
        "    return lr_price, rf_price, dl_price\n",
        "\n",
        "# TEST CASE 1: Nh√† cao c·∫•p\n",
        "print(\"üè† TEST CASE 1: Nh√† Cao C·∫•p\")\n",
        "print(\"150m¬≤, 4PN, 3PT, c√°ch TT 5km, 2 nƒÉm tu·ªïi, ƒë·∫ßy ƒë·ªß ti·ªán √≠ch\")\n",
        "lr1, rf1, dl1 = du_doan_gia_nha(150, 4, 3, 5, 2, 1, 1, 1, 1)\n",
        "print(f\"   Linear Regression: {lr1/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Random Forest: {rf1/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Deep Learning: {dl1/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "\n",
        "# TEST CASE 2: Nh√† b√¨nh d√¢n\n",
        "print(\"\\nüèòÔ∏è TEST CASE 2: Nh√† B√¨nh D√¢n\")\n",
        "print(\"60m¬≤, 2PN, 1PT, c√°ch TT 15km, 10 nƒÉm tu·ªïi, √≠t ti·ªán √≠ch\")\n",
        "lr2, rf2, dl2 = du_doan_gia_nha(60, 2, 1, 15, 10, 0, 0, 0, 0)\n",
        "print(f\"   Linear Regression: {lr2/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Random Forest: {rf2/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Deep Learning: {dl2/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "\n",
        "# TEST CASE 3: Nh√† trung b√¨nh\n",
        "print(\"\\nüè° TEST CASE 3: Nh√† Trung B√¨nh\")\n",
        "print(\"100m¬≤, 3PN, 2PT, c√°ch TT 8km, 5 nƒÉm tu·ªïi, m·ªôt s·ªë ti·ªán √≠ch\")\n",
        "lr3, rf3, dl3 = du_doan_gia_nha(100, 3, 2, 8, 5, 1, 0, 0, 1)\n",
        "print(f\"   Linear Regression: {lr3/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Random Forest: {rf3/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "print(f\"   Deep Learning: {dl3/1_000_000_000:.2f} t·ª∑ VND\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. ƒê√ÅNH GI√Å ƒê·ªò CH√çNH X√ÅC V√Ä K·∫æT LU·∫¨N\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "models = [('Linear Regression', lr_pred), ('Random Forest', rf_pred), ('Deep Learning', dl_pred)]\n",
        "\n",
        "for i, (name, pred) in enumerate(models, 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.scatter(y_test/1_000_000_000, pred/1_000_000_000, alpha=0.6)\n",
        "    plt.plot([y_test.min()/1_000_000_000, y_test.max()/1_000_000_000], \n",
        "             [y_test.min()/1_000_000_000, y_test.max()/1_000_000_000], 'r--')\n",
        "    plt.xlabel('Gi√° Th·ª±c T·∫ø (T·ª∑ VND)')\n",
        "    plt.ylabel('Gi√° D·ª± ƒêo√°n (T·ª∑ VND)')\n",
        "    plt.title(f'{name}\\nR¬≤ = {results[name][\"R2\"]:.3f}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# T√≠nh ƒë·ªô nh·∫•t qu√°n gi·ªØa c√°c models\n",
        "test_cases = [\n",
        "    ('Nh√† Cao C·∫•p', [lr1, rf1, dl1]),\n",
        "    ('Nh√† B√¨nh D√¢n', [lr2, rf2, dl2]),\n",
        "    ('Nh√† Trung B√¨nh', [lr3, rf3, dl3])\n",
        "]\n",
        "\n",
        "print(\"üìä PH√ÇN T√çCH ƒê·ªò NH·∫§T QU√ÅN:\")\n",
        "for case_name, predictions in test_cases:\n",
        "    mean_pred = np.mean(predictions)\n",
        "    std_pred = np.std(predictions)\n",
        "    cv = (std_pred / mean_pred) * 100\n",
        "    print(f\"\\n{case_name}:\")\n",
        "    print(f\"   Gi√° trung b√¨nh: {mean_pred/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "    print(f\"   ƒê·ªô l·ªách chu·∫©n: {std_pred/1_000_000_000:.2f} t·ª∑ VND\")\n",
        "    print(f\"   H·ªá s·ªë bi·∫øn thi√™n: {cv:.1f}%\")\n",
        "    if cv < 10:\n",
        "        print(f\"   ‚úÖ Models d·ª± ƒëo√°n r·∫•t nh·∫•t qu√°n\")\n",
        "    elif cv < 20:\n",
        "        print(f\"   ‚ö†Ô∏è Models d·ª± ƒëo√°n kh√° nh·∫•t qu√°n\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Models d·ª± ƒëo√°n kh√¥ng nh·∫•t qu√°n\")\n",
        "\n",
        "# T√¨m model t·ªët nh·∫•t\n",
        "best_model = max(results.items(), key=lambda x: x[1]['R2'])\n",
        "print(f\"\\nüèÜ MODEL T·ªêT NH·∫§T: {best_model[0]}\")\n",
        "print(f\"   - ƒê·ªô ch√≠nh x√°c (R¬≤): {best_model[1]['R2']:.3f}\")\n",
        "print(f\"   - Sai s·ªë trung b√¨nh: {best_model[1]['Error_%']:.1f}%\")\n",
        "\n",
        "print(\"\\nüí° K·∫æT LU·∫¨N:\")\n",
        "if best_model[1]['R2'] > 0.85:\n",
        "    print(\"‚úÖ Model ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao, c√≥ th·ªÉ tri·ªÉn khai th·ª±c t·∫ø\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è C·∫ßn c·∫£i thi·ªán model ho·∫∑c thu th·∫≠p th√™m d·ªØ li·ªáu\")\n",
        "    \n",
        "print(\"\\nüìö B√ÄI H·ªåC:\")\n",
        "print(\"1. D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng quan tr·ªçng h∆°n thu·∫≠t to√°n ph·ª©c t·∫°p\")\n",
        "print(\"2. Kh√¥ng ph·∫£i l√∫c n√†o Deep Learning c≈©ng t·ªët nh·∫•t\")\n",
        "print(\"3. C·∫ßn test tr√™n nhi·ªÅu scenarios ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô tin c·∫≠y\")\n",
        "print(\"4. C√¢n nh·∫Øc gi·ªØa ƒë·ªô ch√≠nh x√°c v√† kh·∫£ nƒÉng gi·∫£i th√≠ch\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
