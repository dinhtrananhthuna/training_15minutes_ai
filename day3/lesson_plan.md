# Ng√†y 3-4: Tokenization, Embedding v√† LLM Parameters

## üéØ M·ª•c ti√™u h·ªçc t·∫≠p
Sau bu·ªïi h·ªçc n√†y, c√°c b·∫°n s·∫Ω hi·ªÉu:
- C√°ch m√°y t√≠nh chuy·ªÉn ƒë·ªïi text th√†nh s·ªë (tokenization)
- Embedding l√† g√¨ v√† t·∫°i sao quan tr·ªçng
- C√°c parameters ƒëi·ªÅu khi·ªÉn output c·ªßa LLM
- C√°ch √°p d·ª•ng ki·∫øn th·ª©c v√†o th·ª±c t·∫ø

---

## üìö Ph·∫ßn 1: Tokenization - L√†m th·∫ø n√†o m√°y t√≠nh "ƒë·ªçc" text?

### ü§î V·∫•n ƒë·ªÅ c∆° b·∫£n
**M√°y t√≠nh ch·ªâ hi·ªÉu s·ªë, nh∆∞ng ch√∫ng ta mu·ªën n√≥ hi·ªÉu text. L√†m sao?**

### üîç V√≠ d·ª• ƒë∆°n gi·∫£n
```
Text: "Hello world!"
üëá Tokenization
Tokens: ["Hello", " world", "!"]
üëá Convert to numbers
Token IDs: [15496, 995, 0]
```

### üìä C√°c ph∆∞∆°ng ph√°p Tokenization

#### 1. **Word-level Tokenization** (C√°ch ƒë∆°n gi·∫£n nh·∫•t)
```
"I love programming" ‚Üí ["I", "love", "programming"]
```
**V·∫•n ƒë·ªÅ:** 
- T·ª´ m·ªõi kh√¥ng c√≥ trong vocabulary ‚Üí UNK (Unknown)
- Vocabulary r·∫•t l·ªõn (h√†ng tri·ªáu t·ª´)

#### 2. **Character-level Tokenization**
```
"Hello" ‚Üí ["H", "e", "l", "l", "o"]
```
**∆Øu ƒëi·ªÉm:** Vocabulary nh·ªè (ch·ªâ ~100 k√Ω t·ª±)
**Nh∆∞·ª£c ƒëi·ªÉm:** Sequences r·∫•t d√†i, kh√≥ h·ªçc patterns

#### 3. **Subword Tokenization** (Ph∆∞∆°ng ph√°p hi·ªán ƒë·∫°i)

**Byte-Pair Encoding (BPE)** - C√°ch GPT l√†m:
```
B∆∞·ªõc 1: B·∫Øt ƒë·∫ßu v·ªõi characters
"programming" ‚Üí ["p", "r", "o", "g", "r", "a", "m", "m", "i", "n", "g"]

B∆∞·ªõc 2: T√¨m c·∫∑p xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
"mm" xu·∫•t hi·ªán nhi·ªÅu ‚Üí merge th√†nh "mm"
["p", "r", "o", "g", "r", "a", "mm", "i", "n", "g"]

B∆∞·ªõc 3: L·∫∑p l·∫°i cho ƒë·∫øn khi ƒë·ªß vocabulary
"programming" ‚Üí ["program", "ming"]
```

### üí° T·∫°i sao BPE hi·ªáu qu·∫£?
- **T·ª´ th√¥ng d·ª•ng:** ƒê∆∞·ª£c gi·ªØ nguy√™n ("the", "and", "is")
- **T·ª´ hi·∫øm:** ƒê∆∞·ª£c chia nh·ªè ("antidisestablishmentarianism" ‚Üí ["anti", "dis", "establish", "ment", "arian", "ism"])
- **T·ª´ m·ªõi:** V·∫´n c√≥ th·ªÉ tokenize ƒë∆∞·ª£c

---

## üìö Ph·∫ßn 2: Embedding - T·ª´ s·ªë th√†nh √Ω nghƒ©a

### üé® Embedding l√† g√¨?
**Embedding = C√°ch bi·ªÉu di·ªÖn words/tokens d∆∞·ªõi d·∫°ng vectors c√≥ √Ω nghƒ©a**

### üß≠ V√≠ d·ª• tr·ª±c quan
```
Word: "king"
Embedding: [0.2, -0.5, 0.8, 0.1, -0.3, ...]  (vector 768 chi·ªÅu)

T∆∞∆°ng t·ª±:
"queen": [0.3, -0.4, 0.7, 0.2, -0.2, ...]
"man":   [0.1, -0.6, 0.2, 0.4, -0.1, ...]
"woman": [0.2, -0.3, 0.3, 0.5, 0.0, ...]
```

### üßÆ To√°n h·ªçc ƒë·∫±ng sau
```
king - man + woman ‚âà queen
[0.2,-0.5,0.8] - [0.1,-0.6,0.2] + [0.2,-0.3,0.3] ‚âà [0.3,-0.4,0.7]
```

### üìà Evolution c·ªßa Embeddings

#### 1. **Word2Vec (2013)**
- Static embeddings: m·ªói t·ª´ c√≥ 1 vector c·ªë ƒë·ªãnh
- "bank" lu√¥n c√≥ c√πng 1 embedding (d√π l√† ng√¢n h√†ng hay b·ªù s√¥ng)

#### 2. **GloVe (2014)**
- T∆∞∆°ng t·ª± Word2Vec nh∆∞ng h·ªçc t·ª´ co-occurrence statistics

#### 3. **Contextual Embeddings (2018+)**
- BERT, GPT: embedding thay ƒë·ªïi theo context
- "bank" c√≥ embedding kh√°c nhau t√πy theo c√¢u:
  - "I went to the **bank** to withdraw money" (ng√¢n h√†ng)
  - "I sat by the river **bank**" (b·ªù s√¥ng)

### üéØ Token Limits v√† Context Window
```
GPT-3.5: 4,096 tokens (~3,000 t·ª´)
GPT-4:   8,192 tokens (~6,000 t·ª´)
GPT-4-32k: 32,768 tokens (~24,000 t·ª´)
Claude-2: 100,000 tokens (~75,000 t·ª´)
```

**T·∫°i sao c√≥ gi·ªõi h·∫°n?**
- Memory: Attention computation l√† O(n¬≤)
- Cost: X·ª≠ l√Ω nhi·ªÅu tokens = ƒë·∫Øt h∆°n
- Quality: Context qu√° d√†i c√≥ th·ªÉ l√†m gi·∫£m focus

---

## üìö Ph·∫ßn 3: LLM Parameters - ƒêi·ªÅu khi·ªÉn "t√≠nh c√°ch" c·ªßa AI

### üå°Ô∏è Temperature - ƒêi·ªÅu khi·ªÉn Creativity

**Temperature = M·ª©c ƒë·ªô "ng·∫´u nhi√™n" trong l·ª±a ch·ªçn t·ª´ ti·∫øp theo**

```
Prompt: "The weather today is"

Temperature = 0.0 (Deterministic):
"The weather today is sunny and warm."
"The weather today is sunny and warm." (lu√¥n gi·ªëng nhau)

Temperature = 0.3 (Conservative):
"The weather today is pleasant."
"The weather today is beautiful."

Temperature = 0.7 (Balanced):
"The weather today is absolutely gorgeous!"
"The weather today is a bit unpredictable."

Temperature = 1.0 (Creative):
"The weather today is dancing with possibilities!"
"The weather today is whispering secrets to the clouds."

Temperature = 2.0 (Chaotic):
"The weather today is purple elephant mathematics!"
```

### üéØ Top-k Sampling - Gi·ªõi h·∫°n l·ª±a ch·ªçn

**Top-k = Ch·ªâ xem x√©t k t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t**

```
Possible next words v·ªõi probabilities:
"beautiful": 0.4
"sunny": 0.3  
"cloudy": 0.2
"rainy": 0.05
"snowy": 0.03
"terrible": 0.02

Top-k = 2: Ch·ªâ ch·ªçn t·ª´ ["beautiful", "sunny"]
Top-k = 5: Ch·ªçn t·ª´ t·∫•t c·∫£ tr·ª´ "terrible"
```

### üé™ Top-p (Nucleus) Sampling - Dynamic vocabulary

**Top-p = Ch·ªçn t·ª´ ƒë·∫øn khi t·ªïng x√°c su·∫•t ‚â• p**

```
Top-p = 0.9:
"beautiful" (0.4) + "sunny" (0.3) + "cloudy" (0.2) = 0.9 ‚úì
‚Üí Ch·ªçn t·ª´ 3 t·ª´ n√†y

Top-p = 0.7:
"beautiful" (0.4) + "sunny" (0.3) = 0.7 ‚úì
‚Üí Ch·ªâ ch·ªçn t·ª´ 2 t·ª´ n√†y
```

### üîÑ Repetition Penalty
**Gi·∫£m x√°c su·∫•t c·ªßa t·ª´ ƒë√£ xu·∫•t hi·ªán**

```
Without repetition penalty:
"The cat sat on the mat. The cat was happy. The cat..."

With repetition penalty = 1.2:
"The cat sat on the mat. It was content. This feline..."
```

### üå± Seed - Reproducibility
```python
# C√πng prompt + c√πng parameters + c√πng seed = c√πng output
generate_text(
    prompt="Write a function",
    temperature=0.7,
    seed=42  # Lu√¥n cho k·∫øt qu·∫£ gi·ªëng nhau
)
```

---

## üõ†Ô∏è Ph·∫ßn Th·ª±c H√†nh - Jupyter Notebook Demos

### Demo 1: Tokenization Comparison
```python
# So s√°nh c√°c tokenizer kh√°c nhau
from transformers import AutoTokenizer

# Load different tokenizers
gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hello world! This is a test of tokenization methods."

print("GPT-2 Tokenization:")
gpt2_tokens = gpt2_tokenizer.encode(text)
print(f"Tokens: {gpt2_tokens}")
print(f"Decoded: {[gpt2_tokenizer.decode([t]) for t in gpt2_tokens]}")

print("\nBERT Tokenization:")
bert_tokens = bert_tokenizer.encode(text)
print(f"Tokens: {bert_tokens}")
print(f"Decoded: {[bert_tokenizer.decode([t]) for t in bert_tokens]}")
```

### Demo 2: Token Counting
```python
def count_tokens(text, tokenizer_name="gpt2"):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    tokens = tokenizer.encode(text)
    return len(tokens)

# Test v·ªõi different text lengths
texts = [
    "Hi",
    "Hello world!",
    "This is a longer sentence to test tokenization.",
    "This is a much longer paragraph that contains multiple sentences. It should demonstrate how token count increases with text length. We can use this to understand context window limitations."
]

for text in texts:
    token_count = count_tokens(text)
    print(f"Text: '{text[:50]}{'...' if len(text) > 50 else ''}' ‚Üí {token_count} tokens")
```

### Demo 3: Embedding Visualization
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# Load sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Example words/phrases
texts = [
    "king", "queen", "man", "woman",
    "happy", "sad", "joyful", "depressed",
    "cat", "dog", "animal", "pet",
    "car", "vehicle", "transportation", "bike"
]

# Get embeddings
embeddings = model.encode(texts)

# Reduce dimensions for visualization
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings)

# Plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])

# Add labels
for i, txt in enumerate(texts):
    plt.annotate(txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))

plt.title("Word Embeddings Visualization (2D)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True, alpha=0.3)
plt.show()
```

### Demo 4: Parameter Effects
```python
import openai
# Ho·∫∑c s·ª≠ d·ª•ng local model v·ªõi ollama

def test_temperature_effects(prompt, temperatures=[0.0, 0.3, 0.7, 1.0]):
    """Test different temperature settings"""
    results = {}
    
    for temp in temperatures:
        # Simulate API call (replace v·ªõi actual API)
        response = generate_text(
            prompt=prompt,
            temperature=temp,
            max_tokens=50,
            seed=42  # For reproducibility in demo
        )
        results[temp] = response
    
    return results

# Test prompt
prompt = "Write a creative story about a robot learning to cook:"

results = test_temperature_effects(prompt)

for temp, result in results.items():
    print(f"\nTemperature {temp}:")
    print(f"Output: {result}")
    print("-" * 50)
```

### Demo 5: Context Window Testing
```python
def test_context_limits(base_text, max_tokens=4096):
    """Test how models handle long context"""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    
    # Gradually increase text length
    multipliers = [1, 2, 5, 10, 20]
    
    for mult in multipliers:
        text = base_text * mult
        token_count = len(tokenizer.encode(text))
        
        status = "‚úÖ OK" if token_count < max_tokens else "‚ùå Too long"
        print(f"Text length: {len(text)} chars, Tokens: {token_count}, Status: {status}")
        
        if token_count > max_tokens:
            print(f"Exceeded limit by {token_count - max_tokens} tokens")
            break

# Test v·ªõi sample text
sample_text = "This is a sample sentence that we will repeat multiple times to test context window limits. "
test_context_limits(sample_text)
```

---

## üéÆ Hands-on Activities

### Activity 1: Tokenizer Detective
**Nhi·ªám v·ª•:** T√¨m hi·ªÉu t·∫°i sao m·ªôt s·ªë t·ª´ ƒë∆∞·ª£c tokenize l·∫°

```python
# Test v·ªõi nh·ªØng t·ª´ th√∫ v·ªã
test_words = [
    "ChatGPT",           # T√™n ri√™ng
    "antiestablishment", # T·ª´ d√†i
    "üöÄ",                # Emoji
    "caf√©",              # Accented characters
    "hello123",          # Mixed alphanumeric
    "don't",             # Contractions
]

for word in test_words:
    tokens = tokenizer.encode(word)
    decoded = [tokenizer.decode([t]) for t in tokens]
    print(f"'{word}' ‚Üí {decoded} ({len(tokens)} tokens)")
```

### Activity 2: Embedding Math
**Nhi·ªám v·ª•:** Verify famous word analogy: King - Man + Woman = Queen

```python
def word_analogy(model, word1, word2, word3):
    """Solve word1 - word2 + word3 = ?"""
    
    # Get embeddings
    emb1 = model.encode([word1])[0]
    emb2 = model.encode([word2])[0] 
    emb3 = model.encode([word3])[0]
    
    # Calculate result vector
    result_vector = emb1 - emb2 + emb3
    
    # Find closest word (simplified - in practice you'd search a vocabulary)
    candidates = ["queen", "princess", "lady", "woman", "girl", "mother"]
    similarities = []
    
    for candidate in candidates:
        candidate_emb = model.encode([candidate])[0]
        similarity = np.dot(result_vector, candidate_emb) / (
            np.linalg.norm(result_vector) * np.linalg.norm(candidate_emb)
        )
        similarities.append((candidate, similarity))
    
    # Sort by similarity
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"{word1} - {word2} + {word3} = ?")
    print("Top candidates:")
    for word, sim in similarities[:3]:
        print(f"  {word}: {sim:.3f}")

# Test the famous example
word_analogy(model, "king", "man", "woman")
```

### Activity 3: Parameter Playground
**Nhi·ªám v·ª•:** T√¨m combination t·ªët nh·∫•t cho different tasks

```python
# Different tasks require different settings
tasks = {
    "code_generation": {
        "prompt": "Write a Python function to calculate fibonacci:",
        "ideal_params": {"temperature": 0.1, "top_p": 0.9}  # Low creativity
    },
    "creative_writing": {
        "prompt": "Write a fantasy story about a magical forest:",
        "ideal_params": {"temperature": 0.8, "top_p": 0.95}  # High creativity
    },
    "data_analysis": {
        "prompt": "Analyze this sales data and provide insights:",
        "ideal_params": {"temperature": 0.2, "top_p": 0.9}   # Factual
    }
}

def test_parameter_combinations(task_name, task_info):
    prompt = task_info["prompt"]
    ideal = task_info["ideal_params"]
    
    # Test different combinations
    temp_values = [0.1, 0.5, 0.9]
    top_p_values = [0.7, 0.9, 0.95]
    
    print(f"\nTesting task: {task_name}")
    print(f"Ideal parameters: {ideal}")
    print("-" * 50)
    
    for temp in temp_values:
        for top_p in top_p_values:
            # Simulate generation (replace with actual API call)
            result = f"Generated with temp={temp}, top_p={top_p}"
            
            # Calculate "quality score" (simplified)
            distance_from_ideal = abs(temp - ideal["temperature"]) + abs(top_p - ideal["top_p"])
            quality = max(0, 1 - distance_from_ideal)
            
            print(f"temp={temp}, top_p={top_p} ‚Üí Quality: {quality:.2f}")

# Test all tasks
for task_name, task_info in tasks.items():
    test_parameter_combinations(task_name, task_info)
```

---

## üß† Key Takeaways

### 1. **Hi·ªÉu bi·∫øt v·ªÅ Tokenization**
- BPE c√¢n b·∫±ng gi·ªØa hi·ªáu qu·∫£ v√† t√≠nh linh ho·∫°t
- S·ªë l∆∞·ª£ng token ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn chi ph√≠ v√† hi·ªáu su·∫•t
- C√°c m√¥ h√¨nh kh√°c nhau c√≥ tokenizer kh√°c nhau

### 2. **S·ª©c m·∫°nh c·ªßa Embedding**
- C√°c t·ª´ c√≥ nghƒ©a t∆∞∆°ng t·ª± nh√≥m l·∫°i v·ªõi nhau trong kh√¥ng gian vector
- Embedding theo ng·ªØ c·∫£nh t·ªët h∆°n embedding tƒ©nh
- Chi·ªÅu embedding: l·ªõn h∆°n = bi·ªÉu c·∫£m nhi·ªÅu h∆°n nh∆∞ng t·ªën k√©m h∆°n

### 3. **Th√†nh th·∫°o Parameters**
- **Temperature**: 0.0-0.3 cho c√°c t√°c v·ª• th·ª±c t·∫ø, 0.7-1.0 cho c√°c t√°c v·ª• s√°ng t·∫°o
- **Top-p**: Th∆∞·ªùng 0.9-0.95 ho·∫°t ƒë·ªông t·ªët
- **Seed**: Thi·∫øt y·∫øu cho k·∫øt qu·∫£ c√≥ th·ªÉ t√°i t·∫°o trong ki·ªÉm th·ª≠

### 4. **·ª®ng d·ª•ng Th·ª±c t·∫ø**
- T·∫°o code: Temperature th·∫•p + top-p cao
- Vi·∫øt s√°ng t·∫°o: Temperature cao + top-p cao  
- Ph√¢n t√≠ch d·ªØ li·ªáu: Temperature r·∫•t th·∫•p + top-p trung b√¨nh
- Chatbot: Temperature trung b√¨nh + top-p cao

---

## üîÑ Next Steps

1. **Th·ª≠ nghi·ªám** v·ªõi c√°c tokenizer kh√°c nhau tr√™n vƒÉn b·∫£n c·ªßa b·∫°n
2. **Tr·ª±c quan h√≥a** embeddings c·ªßa c√°c thu·∫≠t ng·ªØ chuy√™n ng√†nh
3. **Tinh ch·ªânh** parameters cho c√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng c·ª• th·ªÉ c·ªßa b·∫°n
4. **Gi√°m s√°t** vi·ªác s·ª≠ d·ª•ng token ƒë·ªÉ t·ªëi ∆∞u h√≥a chi ph√≠

---

## üìö Additional Resources

- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/tokenizers)
- [OpenAI Parameters Guide](https://platform.openai.com/docs/api-reference/completions)
- [Sentence Transformers Library](https://www.sbert.net/)
- [BPE Paper](https://arxiv.org/abs/1508.07909)

---

*"Hi·ªÉu v·ªÅ tokenization v√† embeddings l√† n·ªÅn t·∫£ng ƒë·ªÉ th√†nh th·∫°o AI hi·ªán ƒë·∫°i. Parameters l√† c√¥ng c·ª• ƒë·ªÉ ƒëi·ªÅu ch·ªânh h√†nh vi AI theo nhu c·∫ßu c·ªßa b·∫°n."*